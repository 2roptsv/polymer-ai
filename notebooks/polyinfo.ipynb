{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import sys\n",
    "sys.path.append(path.abspath('../'))\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.process_data import DefaultSmilesFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../resources/data/polyinfo.xlsx\")\n",
    "\n",
    "df = pd.read_excel(DATA_PATH.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_properties = set(df.property_name)\n",
    "new_df = defaultdict(lambda: {key: None for key in all_properties})\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    new_df[row[\"polymer_id\"]][row[\"property_name\"]] = row[\"property_value_median\"]\n",
    "    new_df[row[\"polymer_id\"]][\"polymer_id\"] = row[\"polymer_id\"]\n",
    "\n",
    "new_df = pd.DataFrame(list(new_df.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply smiles featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18641"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df.dropna(subset=['SMILES'])\n",
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = DefaultSmilesFeaturizer()\n",
    "\n",
    "smiles_df = new_df[\"SMILES\"].apply(featurizer).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18618"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df[~smiles_df[0].isna()]\n",
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18618"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_df = smiles_df[~smiles_df[0].isna()]\n",
    "len(smiles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18618"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df.drop(columns=[\"SMILES\", 'polymer_id'])\n",
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = [k\n",
    "                  for k, v in dict(new_df.count()).items()\n",
    "                  if v > 1000]\n",
    "TARGET_COLUMNS.remove(\"Electric conductivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = new_df[TARGET_COLUMNS].copy()\n",
    "dfs = {}\n",
    "for column in TARGET_COLUMNS:\n",
    "    target = new_df[column].copy()\n",
    "    index = ~target.isna() & (target < target.quantile(.95)) & (target > target.quantile(.05))\n",
    "    dfs[column] = (target[index], smiles_df[index])\n",
    "\n",
    "print({k: len(v[0]) for k, v in dfs.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x = 3\n",
    "fig, axs = plt.subplots(int(np.ceil(len(TARGET_COLUMNS) / grid_x)), grid_x, figsize=(12, 12))\n",
    "for i, target in enumerate(TARGET_COLUMNS):\n",
    "    ax = axs[int(i / grid_x), i % grid_x]\n",
    "    ax.set_title(target)\n",
    "    ax.hist(dfs[target][0], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras.backend as K\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class QuantileLoss(keras.losses.Loss):\n",
    "    def __init__(self, perc, huber_delta=1e-4, size_alpha=0.):\n",
    "        super().__init__(reduction=keras.losses.Reduction.AUTO, \n",
    "                         name=\"quantile_loss\")\n",
    "        if not isinstance(perc, list):\n",
    "            perc = [perc]\n",
    "        assert all([0 < _q < 1 for _q in perc])\n",
    "        self.__perc = perc\n",
    "\n",
    "        perc = np.array(perc).reshape(-1)\n",
    "        perc.sort()\n",
    "        self._perc = K.constant(perc.reshape(1, -1))\n",
    "\n",
    "        assert 0 < huber_delta < 1\n",
    "        self._delta = huber_delta\n",
    "        self._size_alpha = size_alpha\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"perc\": self.__perc,\n",
    "            \"huber_delta\": self._delta,\n",
    "            \"size_alpha\": self._size_alpha\n",
    "        }\n",
    "\n",
    "    def _huber_quantile_loss(self, y, pred):\n",
    "        I = tf.cast(y <= pred, tf.float32)\n",
    "        d = K.abs(y - pred)\n",
    "        correction = I * (1 - self._perc) + (1 - I) * self._perc\n",
    "        huber_loss = K.sum(correction * tf.where(d <= self._delta, \n",
    "                                                 0.5 * d ** 2 / self._delta, \n",
    "                                                 d - 0.5 * self._delta), -1)\n",
    "        return huber_loss\n",
    "\n",
    "    def call(self, y, pred):\n",
    "        huber_loss = self._huber_quantile_loss(y, pred)\n",
    "        q_order_loss = K.sum(K.maximum(0.0, pred[:, :-1] - pred[:, 1:] + 1e-6), -1)\n",
    "        q_int_size_loss = K.sum(tf.abs(pred[:, 2] - pred[:, 0])) * self._size_alpha\n",
    "        return huber_loss + q_order_loss + q_int_size_loss\n",
    "    \n",
    "@keras.saving.register_keras_serializable()\n",
    "class IntervalAccuracy(tf.keras.metrics.Accuracy):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        matches = tf.logical_and(y_pred[:, 0] < y_true, y_true < y_pred[:, 2])\n",
    "        return super().update_state(tf.ones_like(matches), matches, \n",
    "                                    sample_weight=sample_weight)\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class RelativeIntervalSize:\n",
    "    def __init__(self, mean_value):\n",
    "        self.mean_value = mean_value\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"mean_value\": self.mean_value}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return K.mean(tf.abs(y_pred[:, 2] - y_pred[:, 0])) / self.mean_value\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def q_mae(y_true, y_pred):\n",
    "    return K.mean(tf.abs(y_pred[:, 1] - y_true))\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def q_mape(y_true, y_pred):\n",
    "    return tf.keras.metrics.mean_absolute_percentage_error(\n",
    "        y_true, y_pred[:, 1]\n",
    "    ) / 100\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def mape(y_true, y_pred):\n",
    "    return tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error, accuracy_score, mean_absolute_error, r2_score\n",
    "\n",
    "def plot_loss(history, key_to_label, ymax=0.5):\n",
    "    for k, v in key_to_label.items():\n",
    "        plt.plot(history.history[k], label=v)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, ymax)\n",
    "    plt.grid(True)\n",
    "\n",
    "def plot_target_pred(model, X_test, y_test, ax=None, with_interval=False, verbose=True):\n",
    "    metrics = {}\n",
    "    prediction = []\n",
    "    index = 1 if with_interval else 0\n",
    "\n",
    "    prediction = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    target = y_test.to_numpy()\n",
    "    ind = np.argsort(prediction[:, index])\n",
    "    target = target[ind]\n",
    "    prediction = prediction[ind]\n",
    "\n",
    "    mae = mean_absolute_error(target, prediction[:, index]) / np.mean(np.abs(target))\n",
    "    metrics[\"FMAE\"] = mae\n",
    "    print(\"FMAE: {:.2f}\".format(mae))\n",
    "    r2 = r2_score(target, prediction[:, index])\n",
    "    metrics[\"R2\"] = r2\n",
    "    print(\"R2: {:.2f}\".format(r2))\n",
    "\n",
    "    colors = ['g'] * len(target)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    if with_interval:\n",
    "        matches = np.logical_and(prediction[:, 0] < target,\n",
    "                                 target < prediction[:, 2])\n",
    "        accuracy = np.mean(matches.astype(np.float32))\n",
    "        relative_size = np.mean(np.abs(prediction[:, 0] - prediction[:, 2])) / (np.max(target) - np.min(target))\n",
    "        metrics[\"accuracy\"] = accuracy\n",
    "        metrics[\"mean_relative_interval_size\"] = relative_size\n",
    "        colors = ['g' if x else 'r' for x in matches]\n",
    "\n",
    "        print(\"Mean relative interval size: {:.2f}\".format(relative_size))\n",
    "        print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "        ax.fill_between(x=prediction[:, 1], \n",
    "                        y1=prediction[:, 0], y2=prediction[:, 2], alpha=0.3, color='g')\n",
    "    \n",
    "    ax.set_xlim(np.min(prediction[:, index]), np.max(prediction[:, index]))\n",
    "    ax.set_ylim(np.min(target), np.max(target))\n",
    "    ax.plot(np.array([np.min(target), np.max(target)]), np.array([np.min(target), np.max(target)]), color='r')\n",
    "    ax.scatter(prediction[:, index], target, color=colors, s=1)\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTILES = [0.05, 0.5, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_for_target(ax, target):\n",
    "    data = pd.concat(dfs[target], axis=1)\n",
    "\n",
    "    data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = data_train[smiles_df.columns]\n",
    "    y_train = data_train[target]\n",
    "    X_test = data_test[smiles_df.columns]\n",
    "    y_test = data_test[target]\n",
    "\n",
    "    print(f\"Training model for target {target}.\\nData size {len(X_train)} train, {len(X_test)} test\")\n",
    "\n",
    "    quantile_loss = QuantileLoss(QUANTILES, size_alpha=5e-4)\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(len(QUANTILES)),\n",
    "    ])\n",
    "\n",
    "    mean_interval_size = RelativeIntervalSize(np.mean(y_train))\n",
    "    # IF YOU HAVE M1/M2 MAC\n",
    "    model.compile(loss=[quantile_loss],\n",
    "                  metrics=[q_mape, mean_interval_size, IntervalAccuracy()],\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    \n",
    "    model.fit(X_train, y_train, verbose=0, validation_split=0.2, epochs=300)\n",
    "    \n",
    "    metrics = plot_target_pred(model, X_test, y_test, ax=ax, with_interval=True, verbose=False)\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try one target: Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Density\"\n",
    "data = pd.concat(dfs[target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = data_train[smiles_df.columns]\n",
    "y_train = data_train[target]\n",
    "X_test = data_test[smiles_df.columns]\n",
    "y_test = data_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 316)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "      layers.Dense(16, activation='relu'),\n",
    "      layers.Dense(8, activation='relu'),\n",
    "      layers.Dense(1),\n",
    "  ])\n",
    "\n",
    "# IF YOU HAVE M1/M2 MAC\n",
    "model.compile(loss=[keras.losses.mse],\n",
    "              metrics=[mape, keras.metrics.mae],\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, verbose=0, validation_split=0.2, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history,\n",
    "          {\"val_mape\": \"val MAPE\", \"val_mean_absolute_error\": \"val MAE\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_pred(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_loss = QuantileLoss(QUANTILES, size_alpha=5e-4)\n",
    "q_model = keras.Sequential([\n",
    "      layers.Dense(16, activation='relu'),\n",
    "      layers.Dense(8, activation='relu'),\n",
    "      layers.Dense(len(QUANTILES)),\n",
    "  ])\n",
    "\n",
    "mean_interval_size = RelativeIntervalSize(np.mean(y_train))\n",
    "q_model.compile(loss=[quantile_loss],\n",
    "                metrics=[q_mape, q_mae, mean_interval_size, IntervalAccuracy()],\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "# OTHERWISE\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer=tf.keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = q_model.fit(X_train, y_train, verbose=0, validation_split=0.2, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history,\n",
    "          {\"val_relative_interval_size\": \"mean relative interval size\",\n",
    "           \"val_accuracy\": \"val accuracy\",\n",
    "           \"val_q_mae\": \"val MAE\",\n",
    "           \"val_q_mape\": \"val MAPE\"},\n",
    "           ymax=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_pred(q_model, X_test, y_test, with_interval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_target = {}\n",
    "grid_x = 4\n",
    "fig, axs = plt.subplots(int(np.ceil(len(TARGET_COLUMNS) / grid_x)), grid_x, figsize=(12, 12))\n",
    "for i, target in enumerate(TARGET_COLUMNS):\n",
    "    ax = axs[int(i / grid_x), i % grid_x]\n",
    "    ax.set_title(target)\n",
    "    metrics_per_target[target] = make_model_for_target(ax, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.DataFrame({target: {k: round(v, 2) for k, v in metrics.items()} \n",
    "              for target, (_, metrics) in metrics_per_target.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make table prediction for SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(smiles_string):\n",
    "    smiles_features = np.array([featurizer(smiles_string)])\n",
    "    prediction = []\n",
    "    for i, target in enumerate(TARGET_COLUMNS):\n",
    "        model, metrics = metrics_per_target[target]\n",
    "        model_prediction = model(smiles_features)[0].numpy()\n",
    "        prediction.append({\n",
    "            \"Target\": target,\n",
    "            \"Predicted median\": round(model_prediction[1], 2),\n",
    "            \"Interval\": (round(model_prediction[0], 2), round(model_prediction[2], 2)),\n",
    "            \"Interval accuracy\": round(metrics[\"accuracy\"], 2)\n",
    "        })\n",
    "    return pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "make_prediction(\"*CCC1CC(C2C1CCC2)*\").to_csv(\"prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import NNModelWrapper\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNModelWrapper.save_model(Path(\"../resources/models/nn\"), metrics_per_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = NNModelWrapper(Path(\"../resources/models/nn\"))\n",
    "# Model interface is exactly the same as it was with XGB, see src/model.py\n",
    "# output is {target: [lower bound, median, upper bound]}\n",
    "# interesting metric is accuracy (accuracy of the real value appearance inside interval)\n",
    "\n",
    "# example: [0.5, 0.57, 0.7], 0.8\n",
    "# value is in [0.5, 0.7], error probability 20%\n",
    "mm(\"CCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polymer-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
